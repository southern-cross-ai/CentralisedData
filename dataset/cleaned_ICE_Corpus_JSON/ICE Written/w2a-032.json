{
    "text": "ON THE FEASIBILITY OF USING APPLICATIONSPECIFIC SPEECH TO DERIVE A GENERALPURPOSE SPEECH RECOGNISER TRAINING DATABASE M OKane and P Kenne Faculty of Information Sciences and Engineering University of Canberra ABSTRACT A speech database is easier to markup if what the speakers are saying is known before markingup commences. In a joint project with the court reporting services we have examined the feasibility of using court speech recordings and associated transcript to derive a generalpurpose speech recogniser training database. The first question addressed was the size of the natural vocabulary that was covered by daytoday court proceedings. The next question addressed was the frequency of occurrence of the various words and phrases in this vocabulary. We then turned to the issue of how much transcript had to be examined in total in order to get a reasonable number of examples of all the commonlyoccurring words in the vocabulary. All this work was done using automatic analysis of transcript text. Another important aspect of speechdatabase collecting is the overall time it is going to take to markup a database of known size. In order to address this issue we conducted markup speed trials in which several experienced speech database markers were timed for speed of markingup speech from associated transcript. A special software markup system was used which ideally requires only four mouseclicks to mark up and confirm each instance of each word entered in the database. Each marker was markingup at word level only. Quality of markingup was checked for each marker. While the exact minimum amount of data needed to train a very large speech recogniser is unknown, experiments such as the ones described here suggest that the concept of deriving such databases from applicationspecific speech is a very large but not an impossible task. INTRODUCTION Practical use of statisticallybased recognisers requires the generation of large markedup databases to train the recognisers. The collection and markingup of speech databases is a timeconsuming task. Investigating ways of speeding up the collection of speechrecogniser training databases, we examined court transcript data. Court and parliamentary transcript has been used by others for languagemodelling studies Brown, della Pietra, Mercer, della Pietra and Lai, 1992. In Australian courts essentially all proceedings are recorded and the recordings are retained for a statutory period. For most courts written transcript is also produced and retained. We investigated the issue of seeing if the combination of transcript and recording could be used to derive a useable database for training generalpurpose and applicationspecific speech recognisers. VOCABULARY SIZE In order to gain some idea of the vocabulary size and the number of repetitions of items in the vocabulary we considered eight consecutive days of transcript from one ongoing case in one court. The growth in vocabulary size as a function of the number of days of transcript analysed is given in Figure 1a. In Figure 1b this information is presented again on the same scale as the graph of the total number of all words processed as a function of the number of days of transcript analysed. It should be noted that the transcript size for different days varies. It can be seen that the growth in the vocabulary is quite small. In total after processing eight days of transcript 170,638 words the vocabulary is If all words were repeated equally often this would mean that the average repetitions per word was approximately Of course all words do not occur equally often. So we next investigated the highfrequency words and the number of times they were repeated. The ten highestfrequency words and their number of occurrences are given in Table Note that the word the accounts for 6 of all the words processed and the words ranked 210 in the top ten occurring word list together account for another 5 of all words processed. We were interested not only in the number of items in the vocabulary but also the number of word pairs, word triples, word quadruples and word quintuples that could occur in the transcript and how these numbers varied as each new days transcript was added to the collection. This information is presented in Figure For comparison with Table 1 the top ten occurring word pairs are given in Table The top ten words even though they do occur very often do not provide a very useful vocabulary, taken as a set. In order to see if there are enough repetitions of enough other words we considered, for the eight days of transcript, how many words had greater than 10000, 5000, 2000, 1000, 500, 200, 100 occurrences. This information is given in Table We were also interested in finding out how long it took to get a reasonable number of words of reasonably high repetition rate. Some idea of this is given in Figure 3 in which the total number of words processed divided by the number of words occurring more than 400 times is plotted as a function of number of transcript days. It is interesting that after day 4 102,255 words processed, the ratio starts to rise, indicating that the highfrequency words are occurring more frequently but not a lot of other words are occurring more frequently. GENERAL VERSUS SPECIFIC One would expect that collecting a lot of data from one jurisdiction would lead to a large number of jurisdictionspecific words. Accordingly we investigated for the top 10, 20, 30, 40 and 50 occurrences of the vocabulary, the word pairs, the word triples, the word quadruples and the word quintuples what the ratio of general words or phrases was to the total number of words or phrases for each set. This information is given graphically in Figure The single words are almost totally general at this part of the file with the number of jurisdictionspecific phrases becoming more frequent as the phrase gets longer. SPEAKER VARIETY The number of speakers contributing to any given transcript can vary enormously. For the eight days considered in the experiment described above, only five speakers a judge and four counsel contributed to the first seven days while fourteen speakers contributed to the eighth day which was the day on which witnesses were called for the first time. HOW REPRESENTATIVE IS MATERIAL FROM ONE COURT In order to see if data from a single court was comparable to data from another source we analysed two days of conference proceedings and compared data from these with data from the two longest days day 3 and day 4 of the eight day sequence described above. This information is given in Table MARKUP TIME The issue that makes this series of experiments worthwhile is that a courtderived speech database is extremely easy and quick to markup using a special software tool in which the speech appears in one window and the associated transcript in another. To markup a word, the operator uses only four mouse clicks one to mark the start of the word, one to mark the end, one to press to play the word and thus aurally confirm the start and end, and one in the transcript window to save the marks. Markup trials by various markers on long speech files indicate that a skilled and accurate marker averages about 14 seconds to markup a word in this way. This allows for fixing errors and scrolling both windows when necessary. HOW FEASIBLE Although the returns are considerable, the markingup of court transcript is still a daunting task. For example it would take an operator 16 12 weeks to markup the eight days of transcript, assuming the markingup rate of 14 seconds per word and a 40hour working week. Where one starts to win however is by a bootstrapping technique, whereby initially all words are marked but when enough repetitions whatever enough means of any word or phrase have been marked these are used to train a statistical wordspotter which then marksup these words and phrases automatically. ACKNOWLEDGEMENT This data analysed in this paper was provided by Auscript, the Commonwealth Reporting Service. REFERENCE Brown, P, della Pietra, V,Mercer, R L, della Pietra, S A and Lai J C 1992. An estimate of an upper bound for the entropy of English, Comp. Ling, 18, 31 DETERMINATION OF TRAINING SET SIZE FOR A STATISTICALLYBASED WORDSPOTTER M OKane, P Kenne and O White Faculty of Information Sciences and Engineering University of Canberra ABSTRACT Wordspotting in continuous speech is useful for automatically locating words for audio indexing purposes. Wordspotting is also the basic technology behind concept spotting, in which the location of enough members of a set of semanticallyrelated words and phrases in a particular segment of speech is taken as an indication that the concept represented by that set is being discussed. A set of experiments was conducted as a first attempt to determine the size of the database needed to train a statisticallybased wordspotter. False negatives and the false positives are both treated as errors in wordspotting. In the first experiment the size of the wordspotter training set needed was examined for the speech of a single speaker. Sufficient training data were collected until good wordspotting was achieved for this speaker. This experiment was then repeated for the speech of another speaker so that the variation of training set size as a function of speaker could be investigated. The training sets for the speakers were then pooled and the wordspotter was tested on test sentences for these speakers. The obvious generalisation experiment was then carried out in which the wordspotter was tested on testspeakers who were not in the training set. INTRODUCTION A computationally efficient wordspotter was developed to perform at over 99 recognition in speakerdependent mode. Results less good for the wordspotter working speakerindependently are also presented. THE WORDSPOTTER The wordspotter was constructed as follows. Twelve different 14band broad encodings OKane 1987 were computed from the fft of the input speaker. The bands for any one encoding are mutually exclusive. Each band receives one of the labels a,b,c,dottedline,n. Speech which has the word to be spotted markedup is then encoded using the twelve encodings. For each encoding a dictionary containing the encodings of the markedup target words is constructed. An example of the dictionary for the first encoding for the word crosstalk occurring 119 times in continuous speech is given in Figure If one examines the twelve dictionaries one sees that the entries in any one dictionary are generally close in some wordnearness sense. This can be quantified by formally developing measures which reflect the number of letters by which any two dictionary entries differ. Or it can be quantified by considering all word pairs, triples, dottedline, ntuples within the words of any dictionary. Table 1 shows the number of pairs to 6tuples encountered for the dictionary in Figure This table also gives the number of ntuples theoretically possible for n1 to nThe striking feature of these numbers is how small they are compared to the theoretical possibilities. That this holds for all twelve dictionaries can be seen in Table 2 which gives the 4tuple results for all encodings. This relatively low number of ntuples is used in the wordspotter as follows. The utterance to be tested for the presence of the word to be spotted is encoded using the twelve encodings. Each utterance encoding is searched for the presence of the allowed 4tuples for that encoding. Where two or more 4tuples occur overlapping a potential find is marked. Note that no word in the dictionaries is of length less than five. Also marked are cases of a 4tuple a letter 2 or more overlapping 4tuples and 2 or more overlapping 4tuples a letter a 4tuple and so on. This process is illustrated in Figure The potential finds for the twelve encodings are then ORed in time. Each potential find in each encoding is assigned a notional weight of When two or more finds are ORed the weights are summed. After the ORing process is complete, all portions of speech which have a weight of 7 or more it could be any number up to 12 are deemed to be the wordspotters best attempt at the word being spotted."
}