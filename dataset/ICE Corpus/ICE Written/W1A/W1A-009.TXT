<W1A-009>  <2022>

<I><#\><h>WHAT IS SPEECH TECHNOLOGY ?</h>

 
<p><#\><bold>A</bold>s children we develop speech over a number of years with the majority of the population having no problem breaking speech into separate words and comprehending their meaning. <#\>Also, the English alphabet can be broken into 26 letters reinforcing the appearance that speech is simple. <#\>However, speech is not just letters joined into words. <#\>When we speak, much more is conveyed besides a word's literal content, such as the type of speaker, expression, connotations, emotions and the other information contained in what they are saying. <#\>By looking at the utterances of a single word by different speakers it can be seen that although the utterance is the same, the sound appears different over time. <#\>Differences also occur when the word is placed in sequence amongst other words and not as a single utterance. <#\>(Appendix 1)  <#\>However, humans can still understand these utterances even when they are spoken in different accents or distorted by background noise and other unnatural variables such as attenuation and psychoacoustic effects. <#\>Regardless of variabilities, there must be something in a speech signal that remains unchanged because we as humans can interpret its meaning.</p>
<p><#\>The human ear is a complex speech recognisor.<#\>(Appendix 2). <#\>Sound is funnelled from the outer ear to the eardrum. <#\>The vibrations on the eardrum are transmitted to the inner ear (the cochlea) via three small bones the last which is fixed to a small membrane. <#\>The auditory transducer that converts sound energy into nerve stimuli is the 'Organ of Corti' which is attached to the basilar membrane at the beginning of the cochlea. <#\>It contains specialised and complex hair cells which form the transduction process between the mechanical vibrations of the basilar membrane and nerve impulses transmitted through the brain. <#\>Not as much is known about the ear as the human voice production tools and there are many operations of the ear that are not clear.[6]</p>

<p><#\>As a result, speech recognition by a computer is not a simple decoding problem but a highly complex one.
<#\>The method by which humans produce speech is also a highly involved process. <#\>The sounds of speech are the acoustic consequences of air flowing through the vocal tract. <#\>Air is forced from the lungs and flows through the larynx. <#\>Appropriate tension in the vocal chords causes them to oscillate, in turn exciting the acoustic resonances of the nasal and oral passages to form vowel, vowel-like sounds and consonants. <#\>The articulators of sound are the tongue, velum, teeth, jaw, lips and nasal tract. <#\>'Plosives' are formed by closing off the vocal tract by the lips, tongue or teeth and then releasing the sound energy in a burst. <#\>(Appendix 2)  <#\>Although the vocal tract is capable of producing an infinite number of distinct sounds, at the linguistic level the basic perceptual unit of speech is the phoneme which depends on the word being produced and the position of the phoneme within the utterance. <#\>The English language consists of 44 phonemes.</p>  

<p><#\><bold>T</bold>he concept of a talking machine has been around many years. <#\>The first speech synthesiser existed as early as 1791 where air from bellows was used to excite a resonant cavity to produce vowels and consonants. <#\>The first electrical version appeared in 1939, where the VODER (voice operated demonstrator) utilised a keyboard to generate vowel and consonant-like sounds and control articulation. <#\>A modern speech synthesiser is based on the same principles but uses a digital computer and special digital signal processing (DSP) hardware. <#\>The modern devices synthesise speech automatically from text. <#\>The ultimate speech synthesiser will read aloud from any text form in a clear, natural sounding voice. <#\>However present technology does not quite perform this function.</p>

<p><#\>For many years speech was considered to be composed of linear sequences of the elemental speech particles, the phoneme, put together rather like a string of beads. <#\>This incorrectly led people to believe that speech could be recognised by just decoding the individual phonemes as they appeared through time. <#\> Consequently, it appeared that synthetic speech could be produced by storing each of the phonemes of the language and sticking them together to create a word. <#\> Unfortunately, this proved not to be the case. <#\> While it is convenient to picture speech as a set of discrete symbols real linguistic information is not encoded in discrete packets of time as imagined by this model.</p>

<p><#\>Despite theses limitations, speech output by computers has generally been achieved by generating messages from stored speech fragments. <#\>These messages require recording by a speaker and although a natural sounding voice output is achieved, large amounts of data storage are required thus speech coding is essential. <#\>Also, if messages have to be altered, total re-recording is required if the original speaker is not available. <#\>Many applications would require regular changing of messages so the only efficient solution is a full text to speech system.<#\>[8]</p>
<p><#\>Speech synthesis can be of two forms. <#\>Speech can be generated from artificial sounds to mimic the human utterances or by constructing the voice from the elemental components of a persons pre-recorded speech. <#\>The later system produces a more natural sounding voice but the first is more suited to large and varied responses.</p>   

<p><#\>Printed text consists of alphanumeric characters, blank spaces and punctuation marks. <#\>These characters are a symbolic representation of a language, not a language itself. <#\>The real meaning of the language is not indicated explicitly in the text. <#\>Not all words have equal importance and phrase structure or focus is not obvious. <#\>Also symbols such as numbers and abbreviations must be interpreted correctly. <#\>Text analysis converts the printed text into linguistic structures that establishes phrasal hierarchy. <#\>Without word hierarchy, sentences would lack focal centres resulting in unnatural speech. <#\>Phrasal structure is conveyed by intonation and duration variation, phrases are assigned different tone levels and marked by pauses and prepausal lengthening of words. <#\>Tone also indicate sentence type, ie question or statement.</p>

<p><#\>To perform proper text analysis a text understanding system is necessary. <#\>Present day technology is limited therefore, the above process is broken into three components of: text normalisation, parsing and intonation and letter to phoneme rules.</p>

<p><#\><bold><*>bullet</*>Text Normalisation</bold> - involves the deciphering of non alphabet symbols such as numbers, special symbols, abbreviations and acronyms. <#\>The preprocessor also assigns parts of speech to each word in the input. <#\>This is important for obtaining phrase focus and correct pronunciation of words. <#\>To improve text preprocessing, large amounts of text must be synthesised, and algorithms changed to correct for preprocessing errors.</p>

<p><#\><bold><*>bullet</*>Parsing and Intonation</bold> - establishes the phrasal hierarchy of the text. <#\>When the sentence structure is clear the sentence can be parsed into proper phrases. <#\>If it is not clear a more neutral intonation is used.</p>

<p><#\><bold>Letter to Phoneme Conversion</bold> - is utilised in giving the computer correct pronounciation of each word. <#\>A large dictionary of stored words can be used and when the word is matched the pronunciation and stress is obtained directly. <#\>If there is no match, words may be constructed from single words in the dictionary or through phonemic transcription. <#\>The pronunciation of the synthesiser can be improved by synthesising large amounts of text and adding or correcting the rules that contribute to the mispronunciation. <#\> Where rules fail, words or proper names can be added to the dictionary.</p>

<p><#\><bold>S</bold>peech coding is the process of converting speech into digital bit strings for the efficient storage and/or transmission over band limited channels. <#\>Speech coders for particular applications are selected according to a trade-off between coding complexity, bit rate and signal quality. <#\>Coders are usually classified into the following three types:</p>
<p><#\><bold><*>bullet</*>Waveform coding</bold> - where the aim is to reproduce the original waveform as accurately as possible. <#\>These coders are not speech specific however they can deal with non-speech signals such as background noise, music and multiple speakers without difficulty. <#\>The cost of this fidelity is a relatively high bit rate.</p>

<p><#\><bold><*>bullet</*>Vocoders</bold> - make no attempt to reproduce the original waveform instead deriving a set a parameters at the encoder that are transmitted and used to control a speech production model at the receiver. <#\>Speech quality, although intelligible, tends to be synthetic and variable between speakers, hence vocoding is not used for telephone network applications.</p>

<p><#\><bold><*>bullet</*>Hybrid</bold> - contains the features of both waveform and vocoders to provide good quality, efficient speech coding. <#\>Rates are between 16kbits/s and 4kbits/s and good quality coding is achieved by <it>analysis through synthesis</it> techniques.</p>

<p><#\>Speech coders are in demand for systems where there is a need to make efficient use of precious radio spectrum, in the case of aeronautical telephony via satellite and digital cordless systems. <#\>Also when cost of storage is a major factor speech coding is used. <#\>Applications include interactive network based voice messaging, private network, public switched telephone network (PSDN) and integrated services digital network (ISDN) videophone applications.</p>

 <p><#\><bold>A</bold>chieving machine generated speech is somewhat of a challenge; designing machines which can fully comprehend speech is even more daunting. <#\>This is why this area of speech technology still poses the greatest task and has researchers throughout the world striving to create the most accurate speech recognisor. <#\>This thesis focuses on speech recognition due to the many challenges that are still evident in the speech recognition field.</p>

<p><#\>When people normally interact with computers they use instruments such as keyboards, mouse-driven menus, touch screens or tonepads. <#\>These methods are artificial and as a result deter many people from interacting with computers. <#\>If a speech recognisor could be devised that was 100% accurate the possible applications are endless. <#\>Then again, this would require machines to be more accurate than humans, as humans cannot be considered 100% accurate in recognising speech, especially in situations where there is a strong accent or noise interference.</p>
<p><#\>Speech recognition is fundamentally a pattern classification task. <#\>The aim is to take the input pattern, the speech signal, and to classify it as a sequence of stored patterns that have previously been learned. <#\>The stored patterns are phonemes or words.</p>  

<p><#\><bold>O</bold>ne of the earliest speech recognisors,designed in 1916, was named Flowers (Appendix 3). <#\>It consisted of a set of filters (electromagnetic resonators) that responded to the strongest frequency in the particular utterance. <#\>The design shows that there was an assumption that each letter of the alphabet was characterised uniquely by a single dominant frequency. <#\>This is not the case, but it was the separation of the task of speech recognition into components by feature extraction fundamental to frequency analysis and classification based on a 'best match' that remains a characteristic of modern speech recognisors.</p> 

<p><#\>The next major development was the Davis, Biddulph and Balashek machine.<#\>(Appendix 4). <#\>Speech was filtered into two frequency bands and the zero crossing rates within each band used as primary features. <#\>The advent of the digital computer made this form of speech recognisor viable in the 1960's as only digital technology provided the memory which was necessary. <#\>In this process the unknown sequence of spectral components was aligned in turn against each known sequence in a 'template' library. <#\>The best fit was considered the correct example. <#\>The problem with these machines was that only very simple processing could only be used at almost every stage and that the sequence of operation was slow. <#\>The operation was slow due to the requirement that the speech was collected until silence was detected, then normalised, matched and finally reported. <#\>As a result there was a slow response time between when the speech was uttered to when the result was reported. <#\>This post utterance delay was later eliminated by a new technique of dynamic programming.</p>

<p><#\>Dynamic programming overcame a majority of the theoretical problems but not all of the practical computational problems. <#\>Instead of waiting for the complete utterance, the distance of every spectral frame in the input from every frame in every template was calculated as it arrived and the accumulated difference along the best path maintained.<#\>(Appendix 5). <#\>The advantages were that matching could take place as the signal arrived, recognition was complete as soon as the word was finished. <#\>Connected word recognition was possible along with word spotting and the very best time alignment was possible between the unknown signal and the known template.</p></I>

